"""
RobotReviewer server
"""

# Authors:  Iain Marshall <mail@ijmarshall.com>
#           Joel Kuiper <me@joelkuiper.com>
#           Byron Wallace <byron@ccs.neu.edu>

import logging, os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from datetime import datetime, timedelta

def str2bool(v):
  return v.lower() in ("yes", "true", "t", "1")

DEBUG_MODE = str2bool(os.environ.get("DEBUG", "true"))
LOCAL_PATH = "robotreviewer/uploads"
LOG_LEVEL = (logging.DEBUG if DEBUG_MODE else logging.INFO)
# determined empirically by Edward; covers 90% of abstracts
# (crudely and unscientifically adjusted for grobid)
NUM_WORDS_IN_ABSTRACT = 450

logging.basicConfig(level=LOG_LEVEL, format='[%(levelname)s] %(name)s %(asctime)s: %(message)s')
log = logging.getLogger(__name__)
log.info("Welcome to RobotReviewer :)")

from robotreviewer.textprocessing.pdfreader import PdfReader
pdf_reader = PdfReader() # launch Grobid process before anything else


from flask import Flask, json, make_response, send_file
from flask import redirect, url_for, jsonify
from flask import request, render_template
from flask_bootstrap import Bootstrap
from robotreviewer.ux.forms import QuestionForm, QuestionForm2

from werkzeug.utils import secure_filename

from flask_wtf.csrf import CsrfProtect
import zipfile

try:
    from cStringIO import StringIO # py2
except ImportError:
    from io import BytesIO as StringIO # py3

from robotreviewer.textprocessing.tokenizer import nlp

''' robots! '''
# from robotreviewer.robots.bias_robot import BiasRobot
from robotreviewer.robots.rationale_robot import BiasRobot
from robotreviewer.robots.pico_robot import PICORobot
from robotreviewer.robots.rct_robot import RCTRobot
from robotreviewer.robots.pubmed_robot import PubmedRobot
# from robotreviewer.robots.mendeley_robot import MendeleyRobot
# from robotreviewer.robots.ictrp_robot import ICTRPRobot
from robotreviewer.robots import pico_viz_robot
from robotreviewer.robots.pico_viz_robot import PICOVizRobot

from robotreviewer.data_structures import MultiDict

from robotreviewer import config
import robotreviewer

import uuid
from robotreviewer.util import rand_id
import sqlite3

import hashlib
import random

import numpy as np # note - this should probably be moved!

app = Flask(__name__,  static_url_path='')
from robotreviewer import formatting
app.secret_key = os.environ.get("SECRET", "super secret key")
# setting max file upload size 100 mbs
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024

csrf = CsrfProtect()
csrf.init_app(app)

Bootstrap(app)


######
## default annotation pipeline defined here
######
log.info("Loading the robots...")
bots = {"bias_bot": BiasRobot(top_k=3),
        "pico_bot": PICORobot(),
        "pubmed_bot": PubmedRobot(),
        # "ictrp_bot": ICTRPRobot(),
        "rct_bot": RCTRobot(),
        "pico_viz_bot": PICOVizRobot()}
        # "mendeley_bot": MendeleyRobot()}

log.info("Robots loaded successfully! Ready...")

#####
## connect to and set up database
#####
rr_sql_conn = sqlite3.connect(robotreviewer.get_data('uploaded_pdfs/uploaded_pdfs.sqlite'), detect_types=sqlite3.PARSE_DECLTYPES)
c = rr_sql_conn.cursor()

c.execute('CREATE TABLE IF NOT EXISTS article(id INTEGER PRIMARY KEY, report_uuid TEXT, pdf_uuid TEXT, pdf_hash TEXT, pdf_file BLOB, annotations TEXT, timestamp TIMESTAMP, dont_delete INTEGER, timespan TEXT DEFAULT 0)')
c.execute('CREATE TABLE IF NOT EXISTS form(id INTEGER PRIMARY KEY, ux_uuid VARCHAR(21), pdf_uuid TEXT, first_question TEXT, second_question VARCHAR(21), third_question VARCHAR(21), flag INTEGER, F1 INTEGER, F2 INTEGER, F3 INTEGER, F4 INTEGER, F5 INTEGER, F6 INTEGER, F7 INTEGER, F8 INTEGER, F9 INTEGER, F10 INTEGER, F11 INTEGER, F12 INTEGER, F13 INTEGER, F14 INTEGER, F15 INTEGER, F16 INTEGER, T1 TEXT)')
c.execute('CREATE TABLE IF NOT EXISTS links(id INTEGER PRIMARY KEY, list_urls TEXT, username TEXT, flag TEXT)')
c.close()
rr_sql_conn.commit()



# lastly wait until Grobid is connected
pdf_reader.connect()

@app.route('/')
def main():
    resp = make_response(render_template('index.html'))
    return resp


@app.route('/ux', methods=['GET', 'POST'])
def ux():
    # Here is the random user ID generated by some random function but
    # can be set to some string to test:
    ux_uuid = rand_id() #ID generated from function in util.py
    print("CHECK")
    print('___________________________')
    print(ux_uuid)
    print('___________________________')

    c = rr_sql_conn.cursor()
    c.execute("SELECT report_uuid, pdf_uuid FROM article WHERE dont_delete='2'")
    a = c.fetchall()

    s = []
    u = 0
    username = ux_uuid
    g = [seq[0]+'/'+seq[1] for seq in a]
    list_urls = random.sample(g, 4)


    #PLACEHOLDER
    display_pdfs = [1,0,0,1]
    random.shuffle(display_pdfs)
    flag = ''.join(map(str, display_pdfs))


    for i in list_urls:
        s.append({"link" : "%s%s%s" % ('/#document/', list_urls[u], '?annotation_type=bias_bot'), "ux_user" : ux_uuid})
        u = u + 1
    p = {}
    p["link_meta"] = s

    c = rr_sql_conn.cursor()
    c.execute("INSERT INTO links(list_urls, username, flag) VALUES(?, ?, ?)", (json.dumps(p), username, str(flag)))
    rr_sql_conn.commit()
    c.close()


    return render_template('ux.html', ux_uuid=ux_uuid)

@app.route('/form1/<ux_uuid>', methods=['GET', 'POST'])
def form1(ux_uuid):
    search_id = ux_uuid
    print('SEARCH ID >>>>>>>>> ::::: ', search_id)
    pdf_uuids = 'Ada'
    f = 2
    form = QuestionForm()

    if request.method == 'POST':
        if form.validate() == False:
            print("VALIDATE ME !")
            return render_template('/form1.html', form=form, ux_uuid=ux_uuid)
        else:
            print("CHECKPOINT BETA")
            print('___________________________')
            print(ux_uuid, type(ux_uuid))
            print(form.first_question.data, type(form.first_question.data))
            print(form.second_question.data, type(form.second_question.data))
            print(form.third_question.data, type(form.third_question.data))
            print(f, type(f))
            mylist = json.dumps(form.first_question)
            print('___________________________')
            c = rr_sql_conn.cursor()
            c.execute("INSERT INTO form(ux_uuid, pdf_uuid, first_question, second_question, third_question, flag) VALUES (?, ?, ?, ?, ?, ?)", (ux_uuid, pdf_uuids, mylist, form.second_question.data, form.third_question.data, f))
            rr_sql_conn.commit()
            c.close()

            #return redirect(url_for('ux'))
            #return render_template('video.html', ux_uuid=ux_uuid)
            return redirect(url_for('video', ux_uuid=ux_uuid))

    elif request.method == 'GET':
        print('path two taken')
        return render_template('form1.html', form=form, ux_uuid=ux_uuid)


    return render_template('form1.html')

@app.route('/video/<ux_uuid>', methods=['GET', 'POST'])
def video(ux_uuid):
    c = rr_sql_conn.cursor()
    c.execute("SELECT list_urls, flag FROM links WHERE username = ?", (ux_uuid,))
    a = c.fetchone()
    links = json.loads(a[0])
    task_ids = list(str(a[1]))

    for i, j in zip(links["link_meta"], task_ids):
        url = i["link"] + '&ux_uuid=' + i["ux_user"] + '&flag=1&task_id=' + j
        break
    print(url)
    return render_template('video.html', ux_uuid=ux_uuid, doc_url=url)


@app.route('/form2/<ux_uuid>', methods=['GET', 'POST'])
def form2(ux_uuid):

    form = QuestionForm2()

    if request.method == 'POST':
        if form.validate() == False:
            print("VALIDATE ME !")
            return render_template('/form2.html', form=form, ux_uuid=ux_uuid)
        else:
            #
            #mylist = json.dumps(form.first_question)
            print('I STORED ALL THE FORM DATA IN THE SQLITE3 DB here - and GO ON TO:')
            return render_template('blank.html', ux_uuid=ux_uuid)

    elif request.method == 'GET':
        print('path two taken')
        return render_template('form2.html', form=form, ux_uuid=ux_uuid)

    return render_template('form2.html')


@app.route('/blank', methods=['GET', 'POST'])
def blank():
    ux_uuid = request.args['ux_uuid']

    form = QuestionForm2()

    if request.method == 'POST':
        if form.validate() == False:
            print("VALIDATE ME !")
            return render_template('/form2.html', form=form, ux_uuid=ux_uuid)
        else:
            #
            #mylist = json.dumps(form.first_question)
            print('I STORED ALL THE FORM DATA IN THE SQLITE3 DB here - and GO ON TO:')
            return render_template('blank.html', ux_uuid=ux_uuid)

    elif request.method == 'GET':
        print('path two taken')
        return render_template('form2.html', form=form, ux_uuid=ux_uuid)

    return render_template('form2.html')

#@app.route('/blank', methods=['GET', 'POST'])
#def blank():
#    ux_uuid = request.args['ux_uuid']
#    return render_template('blank.html', ux_uuid=ux_uuid)







@csrf.exempt # TODO: add csrf back in
@app.route('/upload_and_annotate', methods=['POST'])
def upload_and_annotate():
    # uploads a bunch of PDFs, do the RobotReviewer annotation
    # save PDFs + annotations to database
    # returns the report run uuid + list of article uuids

    report_uuid = rand_id()
    pdf_uuids = []

    uploaded_files = request.files.getlist("file")
    c = rr_sql_conn.cursor()

    blobs = [f.read() for f in uploaded_files]
    filenames = [f.filename for f in uploaded_files]

    articles = pdf_reader.convert_batch(blobs)
    parsed_articles = []
    # tokenize full texts here
    for doc in nlp.pipe((d.get('text', u'') for d in articles), batch_size=1, n_threads=config.SPACY_THREADS, tag=True, parse=True, entity=False):
        parsed_articles.append(doc)


    # adjust the tag, parse, and entity values if these are needed later
    for article, parsed_text in zip(articles, parsed_articles):
        article._spacy['parsed_text'] = parsed_text

    for filename, blob, data in zip(filenames, blobs, articles):
        pdf_hash = hashlib.md5(blob).hexdigest()
        pdf_uuid = rand_id()
        pdf_uuids.append(pdf_uuid)
        data = annotate(data, bot_names=["pubmed_bot", "bias_bot", "pico_bot", "rct_bot", "pico_viz_bot"])
        data.gold['pdf_uuid'] = pdf_uuid
        data.gold['filename'] = filename


        c.execute("INSERT INTO article (report_uuid, pdf_uuid, pdf_hash, pdf_file, annotations, timestamp, dont_delete) VALUES(?, ?, ?, ?, ?, ?, ?)", (report_uuid, pdf_uuid, pdf_hash, sqlite3.Binary(blob), data.to_json(), datetime.now(), config.DONT_DELETE))
        rr_sql_conn.commit()
    c.close()

    return json.dumps({"report_uuid": report_uuid,
                       "pdf_uuids": pdf_uuids})

@app.errorhandler(413)
def request_entity_too_large(error):
    ''' @TODO not sure if we want to return something else here? '''
    return json.dumps({'success':False, 'error':True}), 413, {'ContentType':'application/json'}


@csrf.exempt # TODO: add csrf back in
@app.route('/report_view/<report_uuid>/<format>')
def report_view(report_uuid, format):
    return produce_report(report_uuid, format, download=False)

@csrf.exempt # TODO: add csrf back in
@app.route('/download_report/<report_uuid>/<format>')
def download_report(report_uuid, format):
    report = produce_report(report_uuid, format, download=True)
    strIO = StringIO()
    strIO.write(report.encode('utf-8')) # need to send as a bytestring
    strIO.seek(0)
    return send_file(strIO,
                     attachment_filename="robotreviewer_report_%s.%s" % (report_uuid, format),
                     as_attachment=True)


def get_study_name(article):
    authors = article.get("authors")
    if authors:
        if len(authors) == 1:
            return authors[0]["lastname"] + ", " + \
                authors[0]["forename"] + " " + \
                authors[0]["initials"] + "."
        else:
            return authors[0]["lastname"] + " et al."
    else:
        #import pdb; pdb.set_trace()
        return article['filename'][:20].lower().replace(".pdf", "") + " ..."


def produce_report(report_uuid, reportformat, download=False, PICO_vectors=True):
    c = rr_sql_conn.cursor()
    articles, article_ids = [], []
    error_messages = [] # accumulate any errors over articles
    for i, row in enumerate(c.execute("SELECT pdf_uuid, annotations FROM article WHERE report_uuid=?", (report_uuid,))):
        data = MultiDict()
        data.load_json(row[1])
        articles.append(data)
        article_ids.append(row[0])


    if reportformat=='html' or reportformat=='doc':
        # embeddings only relatively meaningful; do not generate
        # if we have only 1 study.
        if sum([(not article.get('_parse_error', False)) for article in articles]) < 2:
            # i.e. if we have fewer than 2 good articles then skip
            PICO_vectors = False

        pico_plot_html = u""
        if PICO_vectors:
            study_names, p_vectors, i_vectors, o_vectors = [], [], [], []
            p_words, i_words, o_words = [], [], []
            for article in articles:
                if article.get('_parse_error'):
                    # need to make errors record more systematically
                    error_messages.append("{0}<br/>".format(get_study_name(article)))

                else:
                    study_names.append(get_study_name(article))
                    p_vectors.append(np.array(article.ml["p_vector"]))
                    p_words.append(article.ml["p_words"])

                    i_vectors.append(np.array(article.ml["i_vector"]))
                    i_words.append(article.ml["i_words"])

                    o_vectors.append(np.array(article.ml["o_vector"]))
                    o_words.append(article.ml["o_words"])


            vectors_d = {"population":np.vstack(p_vectors),
                         "intervention":np.vstack(i_vectors),
                         "outcomes":np.vstack(o_vectors)}

            words_d = {"population":p_words, "intervention":i_words, "outcomes":o_words}

            pico_plot_html = bots["pico_viz_bot"].generate_2d_viz(study_names, vectors_d, words_d,
                                            "{0}-PICO-embeddings".format(report_uuid))


        #import pdb; pdb.set_trace()
        return render_template('reportview.{}'.format(reportformat), headers=bots['bias_bot'].get_domains(), articles=articles,
                                pico_plot=pico_plot_html, report_uuid=report_uuid, online=(not download),
                                errors=error_messages, reportformat=reportformat)
    elif reportformat=='json':
        return json.dumps({"article_ids": article_ids,
                           "article_data": [a.visible_data() for a in articles],
                           "report_id": report_uuid,
                           })
    else:
        raise Exception('format "{}" was requested but not available'.format(reportformat))

@app.route('/pdf/<report_uuid>/<pdf_uuid>')
def get_pdf(report_uuid, pdf_uuid):
    # returns PDF binary from database by pdf_uuid
    # where the report_uuid also matches
    c = rr_sql_conn.cursor()
    c.execute("SELECT pdf_file FROM article WHERE report_uuid=? AND pdf_uuid=?", (report_uuid, pdf_uuid))
    pdf_file = c.fetchone()
    strIO = StringIO()
    strIO.write(pdf_file[0])
    strIO.seek(0)
    return send_file(strIO,
                     attachment_filename="%s.pdf" % pdf_uuid,
                     as_attachment=False)


@app.route('/marginalia/<report_uuid>/<pdf_uuid>', methods=['POST','GET'])
def get_marginalia(report_uuid, pdf_uuid):
    # calculates marginalia from database by pdf_uuid
    # where the report_uuid also matches
    annotation_type = request.args["annotation_type"]
    ux_uuid = request.args["ux_uuid"]
    task_id = int(request.args["task_id"])
    c = rr_sql_conn.cursor()
    c.execute("SELECT annotations FROM article WHERE report_uuid=? AND pdf_uuid=?", (report_uuid, pdf_uuid))
    annotation_json = c.fetchone()
    data = MultiDict()
    data.load_json(annotation_json[0])
    structured_data = []
    if bool(data.ml):
        if len(data.ml["bias"]):
            for row in data.ml["bias"]:
                annotation_metadata = []
                for sent in row["annotations"]:
                    if sent["uuid"] == ux_uuid or (sent["uuid"] == 'default' and task_id == 1):
                        annotation_metadata.append({"content" : sent["content"],
                                                "position" : sent["position"],
                                                "uuid"  : sent["uuid"],
                                                "prefix" : sent["prefix"],
                                                "suffix" : sent["suffix"]})
                find = True
                judge = ""
                for judgement in row["judgement"]:
                    if judgement["uuid"] == ux_uuid:
                        judge = judgement["judgement"]
                        find = False
                    elif find and task_id==1 and judgement["uuid"] == "default":
                        judge = judgement["judgement"]
                structured_data.append({"domain" : row["domain"],
                                    "judgement" : judge,
                                    "annotations" : annotation_metadata})
            data.ml["bias"] = structured_data
    marginalia = bots[annotation_type].get_marginalia(data)
    return json.dumps(marginalia)

@csrf.exempt # TODO: add csrf back in
@app.route('/savemarginalia/<report_uuid>/<pdf_uuid>/<ux_uuid>', methods=['POST'])
def savemarginalia(report_uuid, pdf_uuid, ux_uuid):
    marginalia = request.form['data']
    conn = sqlite3.connect(robotreviewer.get_data('uploaded_pdfs/uploaded_pdfs.sqlite'), detect_types=sqlite3.PARSE_DECLTYPES)
    c = conn.cursor()
    c.execute("SELECT annotations FROM article WHERE report_uuid=? AND pdf_uuid=?", (report_uuid, pdf_uuid))
    annotation_json = c.fetchone()
    data = MultiDict()
    data.load_json(annotation_json[0])
    structured_data_d = []
    for row_d in data.ml['bias']:
        annotation_d_metadata = []
        for sent in row_d["annotations"]:
            if sent["uuid"] != ux_uuid:
                annotation_d_metadata.append({"content" : sent["content"],
                                        "position" : sent["position"],
                                        "uuid"  : sent["uuid"],
                                        "prefix" : sent["prefix"],
                                        "suffix" : sent["suffix"]})
        judgement_data = []
        for judgement in row_d["judgement"]:
            if judgement["uuid"] != ux_uuid:
                judgement_data.append({"uuid" : judgement["uuid"],
                                        "judgement" : judgement["judgement"]})
        structured_data_d.append({"judgement" : judgement_data,
                                    "annotations" : annotation_d_metadata})
    datam = MultiDict()
    datam.load_json(marginalia)
    structured_data = []
    for row in datam.marginalia:
        bias_class = row["description"].replace("**Overall risk of bias prediction**:","")
        if bias_class=="high" or bias_class=="unclear" or bias_class=="low":
            bias_class = bias_class
        else:
            bias_class = ""
        annotation_metadata = []
        for sent in row["annotations"]:
            if sent["uuid"] == ux_uuid:
                annotation_metadata.append({"content" : sent["content"],
                                            "position" : sent["position"],
                                            "uuid"  : sent["uuid"],
                                            "prefix" : sent["prefix"],
                                            "suffix" : sent["suffix"]})
        judgement_data_d = [{"uuid" : ux_uuid,
                                "judgement" : bias_class}]
        structured_data.append({"domain" : row["title"],
                                "judgement" : judgement_data_d,
                                "annotations" : annotation_metadata})
    structured_data_all = []
    for i , j in zip(structured_data, structured_data_d):
        structured_data_all.append({"domain" : i["domain"],
                                    "judgement" : i["judgement"] + j["judgement"],
                                    "annotations" : i["annotations"] + j["annotations"]})
    # ml = bots['bias_bot'].get_marginalia(datam)
    data.ml['bias'] = structured_data_all
    cn = conn.cursor()
    cn.execute("UPDATE article SET annotations = ? WHERE report_uuid=? AND pdf_uuid=?", (data.to_json(), report_uuid, pdf_uuid))
    conn.commit()
    conn.close()
    return json.dumps(datam.to_json())

@csrf.exempt # TODO: add csrf back in
@app.route('/submit_time/<report_uuid>/<pdf_uuid>/<ux_uuid>', methods=['POST'])
def submit_time(report_uuid, pdf_uuid, ux_uuid):
    spent_time = int(request.form['data'])
    time_data = []
    conn = sqlite3.connect(robotreviewer.get_data('uploaded_pdfs/uploaded_pdfs.sqlite'), detect_types=sqlite3.PARSE_DECLTYPES)
    c = conn.cursor()
    c.execute("SELECT timespan FROM article WHERE report_uuid=? AND pdf_uuid=?", (report_uuid, pdf_uuid))
    elapse = c.fetchone()
    time_data = []
    if elapse[0]:
        t_data = json.loads(elapse[0])
        has_entry = False
        for row in t_data:
            if row["uuid"] == ux_uuid:
                total_time = spent_time
                has_entry = True
            else:
                total_time = row["time"]
            time_data.append({"uuid" : row["uuid"],"time" : total_time})
        if has_entry:
            pass
        else:
            time_data.append({"uuid" : ux_uuid, "time" : spent_time})
    else:
        time_data.append({"uuid" : ux_uuid,"time" : spent_time})
    c.execute("UPDATE article SET timespan = ? WHERE report_uuid=? AND pdf_uuid=?", (json.dumps(time_data), report_uuid, pdf_uuid))
    conn.commit()
    conn.close()
    return json.dumps(spent_time)

@csrf.exempt # TODO: add csrf back in
@app.route('/get_time/<report_uuid>/<pdf_uuid>/<ux_uuid>', methods=['POST'])
def get_time(report_uuid, pdf_uuid, ux_uuid):
    c = rr_sql_conn.cursor()
    c.execute("SELECT timespan FROM article WHERE report_uuid=? AND pdf_uuid=?", (report_uuid, pdf_uuid))
    elapse = c.fetchone()
    t_data = json.loads(elapse[0])
    timespan = 0
    for row in t_data:
        if row["uuid"] == ux_uuid:
            timespan = row["time"]
    return json.dumps(timespan)

@csrf.exempt # TODO: add csrf back in
@app.route('/get_next/<pdf_uuid>', methods=['POST','GET'])
def next_link(pdf_uuid):
    ux_uuid = request.args["ux_uuid"]
    flag = int(request.args["flag"])
    c = rr_sql_conn.cursor()
    c.execute("SELECT list_urls, flag FROM links WHERE username = ?", (ux_uuid,))
    a = c.fetchone()
    links = json.loads(a[0])
    url = str('#')
    task_ids = list(str(a[1]))
    if len(links["link_meta"]) == flag:
        url = '/blank?ux_uuid=' + ux_uuid
    else:
        j = 0
        for i, k in zip(links["link_meta"], task_ids):
            if j == flag:
                flag = flag + 1
                url = i["link"] + '&ux_uuid=' + i["ux_user"] + '&flag=' + str(flag) + '&task_id=' + k
                break
            else:
                j = j + 1
    return json.dumps(url)

@csrf.exempt # TODO: add csrf back in
@app.route('/annotate', methods=['POST'])
def json_annotate():
    """
    processes JSON and returns for calls from web API
    """
    json_data = request.json
    annotations = annotate(json_data)
    return json.dumps(annotations)

def annotate(data, bot_names=["bias_bot"]):
    #
    # ANNOTATION TAKES PLACE HERE
    # change the line below if you wish to customise or
    # add a new annotator
    #
    annotations = annotation_pipeline(bot_names, data)
    return annotations

def annotation_pipeline(bot_names, data):
    for bot_name in bot_names:
        log.debug("Sending doc to {} for annotation...".format(bots[bot_name].__class__.__name__))

        data = bots[bot_name].annotate(data)
        log.debug("{} done!".format(bots[bot_name].__class__.__name__))
    return data

def cleanup_database(days=1):
    """
    remove any PDFs which have been here for more than
    1 day, then compact the database
    """
    log.info('Cleaning up database')
    conn = sqlite3.connect(robotreviewer.get_data('uploaded_pdfs/uploaded_pdfs.sqlite'),
                           detect_types=sqlite3.PARSE_DECLTYPES)

    d = datetime.now() - timedelta(days=days)
    c = conn.cursor()
    c.execute("DELETE FROM article WHERE timestamp < datetime(?) AND dont_delete=0", [d])
    conn.commit()
    conn.execute("VACUUM") # make the database smaller again
    conn.commit()
    conn.close()


try:
    from apscheduler.schedulers.background import BackgroundScheduler

    @app.before_first_request
    def initialize():
        log.info("Initializing clean-up task")
        scheduler = BackgroundScheduler()
        scheduler.start()
        scheduler.add_job(cleanup_database, 'interval', hours=12)

except Exception:
    log.warn("Could not start scheduled clean-up task")
